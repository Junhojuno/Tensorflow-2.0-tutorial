# 처음봐서 당황스러운 tensorflow 2.0의 문법 정리

### tf.GradientTape()
```python
learning_rate = 1e-3
with tf.GradientTape() as tape:
  hypothesis = W * x_data + b
  cost = tf.reduce_mean(tf.square(hypothesis-y_data))

W_grad, b_grad = tape.gradient(cost, [W,b])

W.assign_sub(learning_rate*W_grad) # W - lr*W_gradient
b.assign_sub(learning_rate*b_grad) # b - lr*b_gradient

```
- Gradient Descent는 tf.GradientTape()으로 구현한다.
- 변수(W, b)의 변화를 Tape에 저장/기록한다.
- tape의 gradient메소드를 사용하여 cost에 대한 각 변수의 미분값을 구한다. (W_grad, b_grad)
- 위 코드가 gradient descent 한걸음을 의미
