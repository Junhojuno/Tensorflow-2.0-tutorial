# 처음봐서 당황스러운 tensorflow 2.0의 문법 정리

### tf.enable_eager_execution()
```python

import tensorflow as tf

tf.enable_eager_execution()

```
- eager mode로 실행, 즉시 실행을 위한 코드
- tf.enable_eager_execution()를 상단부에 적어주면 sess.run을 할 필요가 없어진다고 한다.
- tensorflow 2.0에선 필요없음. 안써도 sess.run필요없이 잘됨.

### tf.function
```python
# A function is like an op

@tf.function
def add(a, b):
  return a + b

add(tf.ones([2, 2]), tf.ones([2, 2]))  #  [[2., 2.], [2., 2.]]

```
- A tf.function you define is just like a core TensorFlow operation: You can execute it eagerly; 
- you can use it in a graph; it has gradients; and so on.

### tf.GradientTape()
```python
learning_rate = 1e-3
with tf.GradientTape() as tape:
  hypothesis = W * x_data + b
  cost = tf.reduce_mean(tf.square(hypothesis-y_data))

W_grad, b_grad = tape.gradient(cost, [W,b])

W.assign_sub(learning_rate*W_grad) # W - lr*W_gradient
b.assign_sub(learning_rate*b_grad) # b - lr*b_gradient

```
- Gradient Descent는 tf.GradientTape()으로 구현한다.
- 변수(W, b)의 변화를 Tape에 저장/기록한다.
- tape의 gradient메소드를 사용하여 cost에 대한 각 변수의 미분값을 구한다. (W_grad, b_grad)
- 위 코드가 gradient descent 한걸음을 의미
